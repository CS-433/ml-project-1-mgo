{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns #visualisation\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps:\n",
    "1. Run and tune all methods on the full unchanged dataset and compare them\n",
    "-- Use Grid Search for fitting ployn ...\n",
    "2. Do feature selection, data cleaning, and so on - compare improvements\n",
    "-- Do visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import *\n",
    "from helpers_perso import *\n",
    "from plots import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load unchanged dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from zipfile import ZipFile\n",
    "\n",
    "DATA_TRAIN_PATH = '../data/train.csv' # TODO: download train data and supply path here \n",
    "with ZipFile('../data/train.csv.zip', 'r') as zip:\n",
    "    zip.extractall('../data')\n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX = standardize(tX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change labels which are -1 to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example [ 1. -1. -1. -1. -1. -1.  1.  1. -1.  1.]\n",
      "Example after [1. 0. 0. 0. 0. 0. 1. 1. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Binary problem change the labels in y \n",
    "print(\"Example\", y[0:10])\n",
    "y = np.where(y == -1, 0, y)\n",
    "print(\"Example after\", y[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with the unchanged dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Train, Validate and a Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-affe07deb410>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# from train take 20% as a validation set and 10% as a test set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplit_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mratio\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Size of the train set: {}. Size of the validation set: {}.\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtX_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\EPFL\\ml-project-1-mgo\\project1\\scripts\\helpers_perso.py\u001b[0m in \u001b[0;36msplit_data\u001b[1;34m(x, y, ratio, seed)\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0midx_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx_split\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;31m# create split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m     \u001b[0mx_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx_train\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m     \u001b[0mx_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx_val\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx_train\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "# from train take 20% as a validation set and 10% as a test set\n",
    "tX_train, tX_val, y_train, y_val = split_data(tX, y, ratio=0.8, seed=1)\n",
    "print(\"Size of the train set: {}. Size of the validation set: {}.\".format(tX_train.shape[0], tX_val.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download train data and supply path here \n",
    "with ZipFile('../data/test.csv.zip', 'r') as zip:\n",
    "    zip.extractall('../data')\n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "print(\"Size of the test set: {}.\".format(tX_test.shape[0]))\n",
    "# Standardize\n",
    "tX_test = standardize(tX_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with least squares gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training on the learning dataset\n",
    "\n",
    "#train_loss, weights = logistic_regression(y, tX, initial_w=[1] * tX.shape[1], max_iter=5000, gamma=0.5)\n",
    "train_loss, weights = least_squares_GD(y_train, tX_train, initial_w=[0] * tX_train.shape[1], max_iters=50, gamma=0.0001)\n",
    "# Estimating the predictions on the validation set\n",
    "# pred_val = tX_val@weights\n",
    "pred_val = predict_labels(weights, tX_val) #### using -1!!!\n",
    "# Confusion matrix\n",
    "tp, tn, fp, fn = calc_rates(y_val, pred_val)\n",
    "vis_conf_mtx(conf_matrix(tp, tn, fp, fn))\n",
    "# Recall, Precision, F2-Score, Accruacy\n",
    "f_score(recall(tp, fn), precision(tp, fp))\n",
    "accruacy(tp, tn, fp, fn)\n",
    "# Computing the mse resulting from the validation set\n",
    "### odd: just predicted now again\n",
    "mse_test = compute_loss_mse(y_val, tX_val, weights)\n",
    "print(\"MSE loss\", mse_test)\n",
    "print(\"Prediction example\", pred_val[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from helpers_perso import standardize\n",
    "#Let's see the dimension of the input and output\n",
    "#tx, mean_x, std_x = standardize(tX)\n",
    "#tX.shape, mean_x, std_x, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exploring y\n",
    "sns.countplot(x = y, palette = \"bwr\") \n",
    "plt.xlabel(\"Output y values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can remark that the data are not well balanced for the output and this can cause some trouble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "print(tX.shape[1])\n",
    "number_features = tX.shape[1]\n",
    "for i in range(0, number_features):\n",
    "    ax = fig.add_subplot(10, 3, i+1)\n",
    "    #remove useless values\n",
    "    tX_tmp = tX[:, i]\n",
    "    tX_tmp = tX_tmp[tX_tmp > -999]\n",
    "    plt.boxplot(tX_tmp)\n",
    "    plt.xlabel(\"Feature \" + str(i))\n",
    "fig.set_size_inches(20, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "print(tX.shape[1])\n",
    "number_features = tX.shape[1]\n",
    "for i in range(0, number_features):\n",
    "    ax = fig.add_subplot(10, 3, i+1)\n",
    "    #remove useless values\n",
    "    tX_tmp = tX[:, i]\n",
    "    tX_tmp = tX_tmp[tX_tmp > -999]\n",
    "    ax.scatter(tX_tmp, tX_tmp, marker= \"*\", color= \"r\")\n",
    "    ax.set_xlabel(\"Feature \" + str(i))\n",
    "    ax.set_ylabel(\"Feature \" + str(i))\n",
    "fig.set_size_inches(20, 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "Thoses graphs shows the distribution of the features. \n",
    "We can see directly that a feature is categorical :\n",
    "- Feature with 4 categories : 22\n",
    "\n",
    "For the other features, we can see that only a few of them seems well distributed whereas the other have some outliers :\n",
    "- Features with Outliers : 0, 1, 2, 3, 5, 8, 9, 10, 13, 16, 19, 21, 26, 29\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature processing \n",
    "\n",
    "Feature processing : Cleaning the dataset by removing useless features and values, combining others, finding better representations of the features to feed your model, scaling the features, and so on. Check this article\n",
    "on feature engineering: \n",
    "\n",
    "http://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing useless features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX\n",
    "tX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing useless features\n",
    "\n",
    "# We begin by removing all the line with a PRI_jet_all_pt equal to 0\n",
    "#print(tX.shape, y.shape, ids.shape)\n",
    "#line_array = np.int_([])\n",
    "#for i in range (tX.shape[0]):\n",
    "#    if tX[i, 29] == 0. :\n",
    "#        line_array = np.append(line_array, i)\n",
    "#tX = np.delete(tX, line_array, axis=0)\n",
    "#y = np.delete(y, line_array, axis=0)\n",
    "#ids = np.delete(ids, line_array, axis=0) \n",
    "#tX = tX[np.where(tX[:, 29] != 0.)]\n",
    "#print(tX[:15,:])\n",
    "#tX.shape, y.shape, ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Useless feature with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing rows with missing values (Nan)\n",
    "\n",
    "#tX = tX[~np.isnan(tX).any(axis=1)]\n",
    "#print(tX[:15,:])\n",
    "#tX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation between features\n",
    "linear_relation = []\n",
    "mat = np.corrcoef(tX, rowvar = 0)\n",
    "#print(mat)\n",
    "for i in range(mat.shape[0]):\n",
    "    for j in range(0, i):\n",
    "        if mat[i, j] > 0.999:\n",
    "            linear_relation.append((j, i, mat[i, j]))\n",
    "linear_relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import *\n",
    "#Visualization for the linear relation between feature\n",
    "fig = plt.figure()\n",
    "k = 1\n",
    "for (i, j, r) in linear_relation : \n",
    "    ax = fig.add_subplot((len(linear_relation)//3)+1, 3, k)\n",
    "    #we keep useless values to see the relation between features when one is -999\n",
    "    ax.scatter(tX[:,i], tX[:,j], marker= \"*\", color= \"r\")\n",
    "    ax.set_xlabel(\"Feature \" + str(i))\n",
    "    ax.set_ylabel(\"Feature \" + str(j))\n",
    "    k += 1\n",
    "fig.set_size_inches(20, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As they are corrolated, we can remove one of the features in the final input \n",
    "column_array = np.int_([])\n",
    "for (i, j, r) in linear_relation :\n",
    "    if not (i in column_array) :\n",
    "        column_array = np.append(column_array, i)\n",
    "    else :\n",
    "        if not (j in column_array) :\n",
    "            column_array = np.append(column_array, j)\n",
    "tX = np.delete(tX, column_array, axis=1)\n",
    "tX.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now work only with 22 features\n",
    "\n",
    "### Corrolation with prediction :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (20, 25))\n",
    "j = 0\n",
    "\n",
    "#Constructing a new matrice for esier plot\n",
    "y_reshaped = y.reshape(y.shape[0], 1)\n",
    "mat = np.concatenate((y_reshaped, tX), axis=1)\n",
    "\n",
    "#print (mat)\n",
    "#print(mat_one)\n",
    "    \n",
    "#print(np.where(mat[:, 0] == -1.))\n",
    "#print(mat[np.where(mat[:, 0] == -1.), 2])\n",
    "for i in range(1, mat.shape[1]):\n",
    "    plt.subplot(6, 4, j+1)\n",
    "    j += 1\n",
    "    mat_minus_one = mat[np.where(mat[:, 0] == -1.), i].flatten()\n",
    "    mat_one = mat[np.where(mat[:, 0] == 1.), i].flatten()\n",
    "    plt.hist(mat_minus_one[np.where(mat_minus_one != -999.)], bins= 50, color='g', label = 'background', alpha=0.5)\n",
    "    plt.hist(mat_one[np.where(mat_one != -999.)], bins=50, color='r', label = 'Higgs Boson', alpha=0.5)\n",
    "    plt.legend(loc='best')\n",
    "fig.suptitle('Features analysis')\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.95)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Corrolation of features with the prediction\n",
    "tX_y_relation = []\n",
    "mat = np.corrcoef(tX, y, rowvar = 0)\n",
    "print(mat)\n",
    "for i in range(mat.shape[0]):\n",
    "    for j in range(0, i):\n",
    "        if mat[i, j] > 0.999:\n",
    "            tX_y_relation.append((j, i, mat[i, j]))\n",
    "print(tX_y_relation)\n",
    "for (i, j, r) in tX_y_relation : \n",
    "    ax = fig.add_subplot((len(tX_y_relation)//3)+1, 3, k)\n",
    "    #we keep useless values to see the relation between features when one is -999\n",
    "    ax.scatter(tX[:,i], tX[:,j], marker= \"*\", color= \"r\")\n",
    "    ax.set_xlabel(\"Feature \" + str(i))\n",
    "    ax.set_ylabel(\"Feature \" + str(j))\n",
    "    k += 1\n",
    "fig.set_size_inches(20, 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing trainer with validation\n",
    "loss, weights = trainer_val(y_val, tX_val, 10, least_squares_GD, compute_loss_mse, y_train, tX_train, [0] * tX_train.shape[1], 55, 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimating the predictions on the validation set\n",
    "# pred_val = tX_val@weights\n",
    "pred_val = predict_labels(weights, tX_val) #### using -1!!!\n",
    "# Confusion matrix\n",
    "tp, tn, fp, fn = calc_rates(y_val, pred_val)\n",
    "vis_conf_mtx(conf_matrix(tp, tn, fp, fn))\n",
    "# Recall, Precision, F2-Score, Accruacy\n",
    "f_score(recall(tp, fn), precision(tp, fp))\n",
    "accruacy(tp, tn, fp, fn)\n",
    "# Computing the mse resulting from the validation set\n",
    "### odd: just predicted now again\n",
    "mse_test = compute_loss_mse(y_val, tX_val, weights)\n",
    "print(\"MSE loss\", mse_test)\n",
    "print(\"Prediction example\", pred_val[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### least_squares_SGD\n",
    "### least_squares\n",
    "### ridge_regression\n",
    "### logistic_regression\n",
    "### reg_logistic_regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with the filtered dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/output.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
